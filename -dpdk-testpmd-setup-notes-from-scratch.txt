```

OUTLINE-Deploying SRIOV based workloads with second-generation AWS Outposts racks.docx

PREP
===
0. CloudFormation to deploy worker node
https://github.com/sigitp-git/outposts-server-eks-worker-node/blob/main/2-eks-worker-node-al2023-longhorn-ebs.yaml
- Add userdata script to enable HugePages, persists after worker node restart
  sudo sed -i 's/selinux=1/& default_hugepagesz=1GB hugepagesz=1G hugepages=32/g' /etc/default/grub
  sudo grub2-mkconfig -o /boot/grub2/grub.cfg
- Add userdata script to create SRIOV-VF, persists after worker node restart


1. Check PCI ID of the Mellanox Cards
[root@ip-10-0-58-16 ~]# lspci | grep Mellanox
0000:05:00.0 Ethernet controller: Mellanox Technologies MT2910 Family [ConnectX-7]
0000:05:00.1 Ethernet controller: Mellanox Technologies MT2910 Family [ConnectX-7]
0001:05:00.0 Ethernet controller: Mellanox Technologies MT2910 Family [ConnectX-7]
0001:05:00.1 Ethernet controller: Mellanox Technologies MT2910 Family [ConnectX-7]
Bring the PF interfaces up and add VFs (automate in user data script/CFN)
--------------------------------------
[root@ip-10-0-58-16 ~]# cat create-virtual-function.sh
#!/bin/bash
yum install -y lshw
INTERFACES=$(lshw -class network -json | jq '.[] | select(.product=="MT2910 Family [ConnectX-7]").logicalname' | tr -d '"')
# max NUMBER_VFS for Mellanox CX-7 is 127
NUMBER_VFS=10
for interface in ${INTERFACES[@]}
do
    echo Updating Virtual Functions for interface: ${interface}
    echo ifconfig ${interface} up
    echo ${NUMBER_VFS} > /sys/class/net/${interface}/device/sriov_numvfs
done
[root@ip-10-0-58-16 ~]#

[root@ip-10-0-58-16 ~]# ./create-virtual-function.sh
Last metadata expiration check: 1 day, 13:16:03 ago on Wed May  7 14:17:27 2025.
Package lshw-B.02.19.2-7.amzn2023.0.3.x86_64 is already installed.
Dependencies resolved.
Nothing to do.
Complete!
Updating Virtual Functions for interface: ens1f0np0
ifconfig ens1f0np0 up
Updating Virtual Functions for interface: ens1f1np1
ifconfig ens1f1np1 up
Updating Virtual Functions for interface: enP1s11f0np0
ifconfig enP1s11f0np0 up
Updating Virtual Functions for interface: enP1s11f1np1
ifconfig enP1s11f1np1 up
[root@ip-10-0-58-16 ~]# ip -d link show | grep vf
    vf 0     link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off
    vf 1     link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off
    vf 2     link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off
    vf 3     link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off
    vf 4     link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off
    vf 5     link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off
    vf 6     link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off
    vf 7     link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off
    vf 8     link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off
    vf 9     link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off
    vf 0     link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off
    vf 1     link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off
    vf 2     link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off
    vf 3     link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off
    vf 4     link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off
    vf 5     link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off
    vf 6     link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off
    vf 7     link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off
    vf 8     link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off
    vf 9     link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off
    vf 0     link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off
    vf 1     link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off
    vf 2     link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off
    vf 3     link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off
    vf 4     link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off
    vf 5     link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off
    vf 6     link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off
    vf 7     link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off
    vf 8     link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off
    vf 9     link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off
    vf 0     link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off
    vf 1     link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off
    vf 2     link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off
    vf 3     link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off
    vf 4     link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off
    vf 5     link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off
    vf 6     link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off
    vf 7     link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off
    vf 8     link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off
    vf 9     link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state auto, trust off, query_rss off
[root@ip-10-0-58-16 ~]#

### For CX-6
root@ip-10-0-127-239:~# cat create-virtual-function.sh 
#!/bin/bash
apt install -y lshw
INTERFACES=$(lshw -class network -json | jq '.[] | select(.product=="MT2892 Family [ConnectX-6 Dx]").logicalname' | tr -d '"')
# max NUMBER_VFS for Mellanox CX-6 is 127
NUMBER_VFS=8
for interface in ${INTERFACES[@]}
do
    echo Updating Virtual Functions for interface: ${interface}
    echo ifconfig ${interface} up
    echo ${NUMBER_VFS} > /sys/class/net/${interface}/device/sriov_numvfs
done
root@ip-10-0-127-239:~#


2. Update SRIOV-DP ConfigMap
edit the sriov-dp-configmap to use the above PCI addresses

15b3 is vendor id
101e is device id for VF, note that SRIOV-DP ConfigMap takes device id for VF, not for PF (1021 for CX-7, 101d for CX-6)

## CX-7
[root@ip-10-0-58-16 ~]# lspci | grep Mell
0000:05:00.0 Ethernet controller: Mellanox Technologies MT2910 Family [ConnectX-7]
0000:05:00.1 Ethernet controller: Mellanox Technologies MT2910 Family [ConnectX-7]

[root@ip-10-0-58-16 ~]# lspci -nk | grep 05.00 -A2
0000:05:00.0 0200: 15b3:1021
        Subsystem: 15b3:0022
        Kernel driver in use: mlx5_core
--
0000:05:00.1 0200: 15b3:1021
        Subsystem: 15b3:0022
        Kernel driver in use: mlx5_core
--
0000:05:00.2 0200: 15b3:101e
        Subsystem: 15b3:0022
        Kernel driver in use: mlx5_core
--
0000:05:00.3 0200: 15b3:101e
        Subsystem: 15b3:0022
        Kernel driver in use: mlx5_core
--
0000:05:00.4 0200: 15b3:101e
        Subsystem: 15b3:0022
        Kernel driver in use: mlx5_core
--
0000:05:00.5 0200: 15b3:101e
        Subsystem: 15b3:0022
        Kernel driver in use: mlx5_core
--
0000:05:00.6 0200: 15b3:101e
        Subsystem: 15b3:0022
        Kernel driver in use: mlx5_core
--
0000:05:00.7 0200: 15b3:101e
        Subsystem: 15b3:0022
        Kernel driver in use: mlx5_core
--
0001:05:00.0 0200: 15b3:1021
        Subsystem: 15b3:0022
        Kernel driver in use: mlx5_core
--
0001:05:00.1 0200: 15b3:1021
        Subsystem: 15b3:0022
        Kernel driver in use: mlx5_core
--
0001:05:00.2 0200: 15b3:101e
        Subsystem: 15b3:0022
        Kernel driver in use: mlx5_core
--
0001:05:00.3 0200: 15b3:101e
        Subsystem: 15b3:0022
        Kernel driver in use: mlx5_core
--
0001:05:00.4 0200: 15b3:101e
        Subsystem: 15b3:0022
        Kernel driver in use: mlx5_core
--
0001:05:00.5 0200: 15b3:101e
        Subsystem: 15b3:0022
        Kernel driver in use: mlx5_core
--
0001:05:00.6 0200: 15b3:101e
        Subsystem: 15b3:0022
        Kernel driver in use: mlx5_core
--
0001:05:00.7 0200: 15b3:101e
        Subsystem: 15b3:0022
        Kernel driver in use: mlx5_core
[root@ip-10-0-58-16 ~]# lspci | grep Mell

## CX-6
root@ip-10-0-127-239:~# lspci | grep Mell
0000:05:00.0 Ethernet controller: Mellanox Technologies MT2892 Family [ConnectX-6 Dx]
0000:05:00.1 Ethernet controller: Mellanox Technologies MT2892 Family [ConnectX-6 Dx]

root@ip-10-0-127-239:~# lspci -nk | grep 05.00 -A2
0000:05:00.0 0200: 15b3:101d
        Subsystem: 15b3:0042
        Kernel driver in use: mlx5_core
--
0000:05:00.1 0200: 15b3:101d
        Subsystem: 15b3:0042
        Kernel driver in use: mlx5_core
--
0000:05:00.2 0200: 15b3:101e
        Subsystem: 15b3:0042
        Kernel driver in use: mlx5_core
--
0000:05:00.3 0200: 15b3:101e
        Subsystem: 15b3:0042
        Kernel driver in use: mlx5_core
--
0000:05:00.4 0200: 15b3:101e
        Subsystem: 15b3:0042
        Kernel driver in use: mlx5_core
--
0000:05:00.5 0200: 15b3:101e
        Subsystem: 15b3:0042
        Kernel driver in use: mlx5_core
--
0000:05:00.6 0200: 15b3:101e
        Subsystem: 15b3:0042
        Kernel driver in use: mlx5_core
--
0000:05:00.7 0200: 15b3:101e
        Subsystem: 15b3:0042
        Kernel driver in use: mlx5_core
--
0003:05:00.0 0280: 177d:bc00
        Subsystem: 177d:bc00
0003:05:00.1 0280: 177d:ef00
        Subsystem: 177d:bc00
0003:07:00.0 0280: 177d:bc00
root@ip-10-0-127-239:~# 


ubuntu@ip-10-0-10-242:~/environment/dpdk-testpmd-bmn-cluster$ kubectl apply -f sriov-dp-configmap-pf-split.yaml
configmap/sriovdp-config created


3. Deploy SRIOV-DP daemonset
Two options of sriovdp-daemonsets upstream:
-----------------------------------------
https://raw.githubusercontent.com/k8snetworkplumbingwg/sriov-network-device-plugin/refs/heads/master/deployments/sriovdp-daemonset.yaml
https://raw.githubusercontent.com/sigitp-git/outposts-server-eks-worker-node/main/sriov-device-plugin-ds-incl-arm64.yaml

internal AWS plugins:
---------------------- 
https://quip-amazon.com/6u2YOm2f14va/Kinara-EKS-Addons
https://quip-amazon.com/xoTfAZF6B85O/Project-Kinara-SRIOV-Addon-Test-10072024
using upstream plugin:

----------------------
## CX-7 on Intel/AMD64
ubuntu@ip-10-0-10-242:~/environment/dpdk-testpmd-bmn-cluster$ kubectl apply -f https://raw.githubusercontent.com/sigitp-git/outposts-server-eks-worker-node/main/sriov-device-plugin-ds-incl-arm64.yaml
serviceaccount/sriov-device-plugin created
daemonset.apps/kube-sriov-device-plugin-amd64 created
daemonset.apps/kube-sriov-device-plugin-ppc64le created
daemonset.apps/kube-sriov-device-plugin-arm64 created
ubuntu@ip-10-0-10-242:~/environment/dpdk-testpmd-bmn-cluster$
-- ubuntu@ip-10-0-10-242:~/environment/dpdk-testpmd-bmn-cluster$ kubectl delete -f https://raw.githubusercontent.com/sigitp-git/outposts-server-eks-worker-node/main/sriov-device-plugin-ds-incl-arm64.yaml
serviceaccount "sriov-device-plugin" deleted
daemonset.apps "kube-sriov-device-plugin-amd64" deleted
daemonset.apps "kube-sriov-device-plugin-ppc64le" deleted
daemonset.apps "kube-sriov-device-plugin-arm64" deleted
ubuntu@ip-10-0-10-242:~/environment/dpdk-testpmd-bmn-cluster$ kubectl get po -A
NAMESPACE                  NAME                                                              READY   STATUS    RESTARTS      AGE
amazon-cloudwatch          amazon-cloudwatch-observability-controller-manager-59c95f7dwspd   1/1     Running   1 (67m ago)   35h
amazon-cloudwatch          cloudwatch-agent-xnzf7                                            1/1     Running   1 (67m ago)   29h
amazon-cloudwatch          fluent-bit-qmqv5                                                  1/1     Running   1 (67m ago)   29h
external-dns               external-dns-7d4c9f944-nmq2j                                      1/1     Running   1 (67m ago)   35h
kube-state-metrics         kube-state-metrics-9c8dc6699-wmqxq                                1/1     Running   1 (67m ago)   35h
kube-system                aws-node-kqwvg                                                    2/2     Running   2 (67m ago)   29h
kube-system                coredns-6b9575c64c-fbnx6                                          1/1     Running   1 (51m ago)   35h
kube-system                coredns-6b9575c64c-x2shr                                          1/1     Running   1 (51m ago)   35h
kube-system                eks-node-monitoring-agent-c7r7q                                   1/1     Running   2 (51m ago)   29h
kube-system                eks-pod-identity-agent-8m4hh                                      1/1     Running   1 (67m ago)   29h
kube-system                kube-proxy-m6hxl                                                  1/1     Running   1 (67m ago)   29h
kube-system                kube-sriov-device-plugin-amd64-mzmj4                              1/1     Running   0             16s
kube-system                metrics-server-6d67d68f67-tdftj                                   1/1     Running   1 (67m ago)   35h
kube-system                metrics-server-6d67d68f67-v7fb7                                   1/1     Running   1 (67m ago)   35h
monitoring                 sriov-metrics-exporter-hfkxd                                      1/1     Running   1 (67m ago)   24h
prometheus-node-exporter   prometheus-node-exporter-tcw6x                                    1/1     Running   1 (67m ago)   29h
ubuntu@ip-10-0-10-242:~/environment/dpdk-testpmd-bmn-cluster$

## VF plumbed for the node
ubuntu@ip-10-0-10-242:~/environment/dpdk-testpmd-bmn-cluster$ kubectl describe node ip-10-0-58-16.ec2.internal
Name:               ip-10-0-58-16.ec2.internal
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/instance-type=bmn-cx2.metal-48xl
                    beta.kubernetes.io/os=linux
                    failure-domain.beta.kubernetes.io/region=us-east-1
                    failure-domain.beta.kubernetes.io/zone=us-east-1a
                    feature.node.kubernetes.io/network-sriov.capable=true
                    is_worker=true
                    k8s.io/cloud-provider-aws=57da6bebdfa05cedd21b9fee26050794
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=ip-10-0-58-16.ec2.internal
                    kubernetes.io/os=linux
                    node.kubernetes.io/instance-type=bmn-cx2.metal-48xl
                    node.longhorn.io/create-default-disk=true
                    storage=longhorn
                    topology.k8s.aws/zone-id=use1-az6
                    topology.kubernetes.io/region=us-east-1
                    topology.kubernetes.io/zone=us-east-1a
Annotations:        alpha.kubernetes.io/provided-node-ip: 10.0.58.16
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 07 May 2025 14:28:53 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  ip-10-0-58-16.ec2.internal
  AcquireTime:     <unset>
  RenewTime:       Fri, 09 May 2025 03:35:56 +0000
Conditions:
  Type                    Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----                    ------  -----------------                 ------------------                ------                       -------
  MemoryPressure          False   Fri, 09 May 2025 03:34:29 +0000   Thu, 08 May 2025 18:40:52 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure            False   Fri, 09 May 2025 03:34:29 +0000   Thu, 08 May 2025 18:40:52 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure             False   Fri, 09 May 2025 03:34:29 +0000   Thu, 08 May 2025 18:40:52 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready                   True    Fri, 09 May 2025 03:34:29 +0000   Thu, 08 May 2025 18:40:52 +0000   KubeletReady                 kubelet is posting ready status
  ContainerRuntimeReady   True    Fri, 09 May 2025 03:31:18 +0000   Thu, 08 May 2025 18:41:17 +0000   ContainerRuntimeIsReady      Monitoring for the ContainerRuntime system is active
  StorageReady            True    Fri, 09 May 2025 03:31:18 +0000   Thu, 08 May 2025 18:41:17 +0000   DiskIsReady                  Monitoring for the Disk system is active
  NetworkingReady         False   Fri, 09 May 2025 03:31:18 +0000   Thu, 08 May 2025 18:46:17 +0000   InterfaceNotUp               Interface "ens1f0np0" is not up
  KernelReady             True    Fri, 09 May 2025 03:31:18 +0000   Thu, 08 May 2025 18:41:17 +0000   KernelIsReady                Monitoring for the Kernel system is active
Addresses:
  InternalIP:   10.0.58.16
  ExternalIP:   3.90.17.61
  InternalDNS:  ip-10-0-58-16.ec2.internal
  Hostname:     ip-10-0-58-16.ec2.internal
  ExternalDNS:  ec2-3-90-17-61.compute-1.amazonaws.com
Capacity:
  amazon.com/bmn-mlx-sriov-pf1:  10
  amazon.com/bmn-mlx-sriov-pf2:  10
  amazon.com/bmn-mlx-sriov-pf3:  10
  amazon.com/bmn-mlx-sriov-pf4:  10
  cpu:                           192
  ephemeral-storage:             104779756Ki
  hugepages-1Gi:                 32Gi
  hugepages-2Mi:                 0
  memory:                        1056599856Ki
  pods:                          737
Allocatable:
  amazon.com/bmn-mlx-sriov-pf1:  10
  amazon.com/bmn-mlx-sriov-pf2:  10
  amazon.com/bmn-mlx-sriov-pf3:  10
  amazon.com/bmn-mlx-sriov-pf4:  10
  cpu:                           191450m
  ephemeral-storage:             95491281146
  hugepages-1Gi:                 32Gi
  hugepages-2Mi:                 0
  memory:                        1014380336Ki
  pods:                          737
System Info:
  Machine ID:                 8207b365efd945af869f804bc6332d85
  System UUID:                ec25d0d9-aec8-19d1-c768-400eb5a30fcf
  Boot ID:                    b3c1ddd1-1b61-41d6-804f-9d302d988d36
  Kernel Version:             6.1.132-147.221.amzn2023.x86_64
  OS Image:                   Amazon Linux 2023.7.20250414
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  containerd://1.7.27
  Kubelet Version:            v1.32.3-eks-473151a
  Kube-Proxy Version:         v1.32.3-eks-473151a
ProviderID:                   aws:///us-east-1a/i-0cf04bdc5a3d77b45
Non-terminated Pods:          (16 in total)
  Namespace                   Name                                                               CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                                               ------------  ----------  ---------------  -------------  ---
  amazon-cloudwatch           amazon-cloudwatch-observability-controller-manager-59c95f7dwspd    100m (0%)     0 (0%)      64Mi (0%)        0 (0%)         43h
  amazon-cloudwatch           cloudwatch-agent-xnzf7                                             250m (0%)     500m (0%)   128Mi (0%)       512Mi (0%)     37h
  amazon-cloudwatch           fluent-bit-qmqv5                                                   50m (0%)      500m (0%)   25Mi (0%)        250Mi (0%)     37h
  external-dns                external-dns-7d4c9f944-nmq2j                                       10m (0%)      0 (0%)      32Mi (0%)        64Mi (0%)      43h
  kube-state-metrics          kube-state-metrics-9c8dc6699-wmqxq                                 100m (0%)     0 (0%)      250Mi (0%)       400Mi (0%)     43h
  kube-system                 aws-node-kqwvg                                                     50m (0%)      0 (0%)      0 (0%)           0 (0%)         37h
  kube-system                 coredns-6b9575c64c-fbnx6                                           100m (0%)     0 (0%)      70Mi (0%)        170Mi (0%)     43h
  kube-system                 coredns-6b9575c64c-x2shr                                           100m (0%)     0 (0%)      70Mi (0%)        170Mi (0%)     43h
  kube-system                 eks-node-monitoring-agent-c7r7q                                    10m (0%)      250m (0%)   30Mi (0%)        100Mi (0%)     37h
  kube-system                 eks-pod-identity-agent-8m4hh                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         37h
  kube-system                 kube-proxy-m6hxl                                                   100m (0%)     0 (0%)      0 (0%)           0 (0%)         37h
  kube-system                 kube-sriov-device-plugin-amd64-g6clx                               250m (0%)     1 (0%)      40Mi (0%)        200Mi (0%)     100s
  kube-system                 metrics-server-6d67d68f67-tdftj                                    100m (0%)     0 (0%)      200Mi (0%)       400Mi (0%)     43h
  kube-system                 metrics-server-6d67d68f67-v7fb7                                    100m (0%)     0 (0%)      200Mi (0%)       400Mi (0%)     43h
  monitoring                  sriov-metrics-exporter-hfkxd                                       100m (0%)     100m (0%)   100Mi (0%)       100Mi (0%)     32h
  prometheus-node-exporter    prometheus-node-exporter-tcw6x                                     100m (0%)     0 (0%)      30Mi (0%)        50Mi (0%)      37h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource                      Requests     Limits
  --------                      --------     ------
  cpu                           1520m (0%)   2350m (1%)
  memory                        1239Mi (0%)  2816Mi (0%)
  ephemeral-storage             0 (0%)       0 (0%)
  hugepages-1Gi                 0 (0%)       0 (0%)
  hugepages-2Mi                 0 (0%)       0 (0%)
  amazon.com/bmn-mlx-sriov-pf1  0            0
  amazon.com/bmn-mlx-sriov-pf2  0            0
  amazon.com/bmn-mlx-sriov-pf3  0            0
  amazon.com/bmn-mlx-sriov-pf4  0            0
Events:
  Type     Reason                   Age                From     Message
  ----     ------                   ----               ----     -------
  Normal   Starting                 33m                kubelet  Starting kubelet.
  Warning  InvalidDiskCapacity      33m                kubelet  invalid capacity 0 on image filesystem
  Normal   NodeHasSufficientMemory  33m (x2 over 33m)  kubelet  Node ip-10-0-58-16.ec2.internal status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    33m (x2 over 33m)  kubelet  Node ip-10-0-58-16.ec2.internal status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     33m (x2 over 33m)  kubelet  Node ip-10-0-58-16.ec2.internal status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  33m                kubelet  Updated Node Allocatable limit across pods
  Normal   Starting                 33m                kubelet  Starting kubelet.
  Warning  InvalidDiskCapacity      33m                kubelet  invalid capacity 0 on image filesystem
  Normal   NodeHasSufficientMemory  33m (x2 over 33m)  kubelet  Node ip-10-0-58-16.ec2.internal status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    33m (x2 over 33m)  kubelet  Node ip-10-0-58-16.ec2.internal status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     33m (x2 over 33m)  kubelet  Node ip-10-0-58-16.ec2.internal status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  33m                kubelet  Updated Node Allocatable limit across pods
ubuntu@ip-10-0-10-242:~/environment/dpdk-testpmd-bmn-cluster$

----------------------
## CX-6 on Graviton/ARM64
ubuntu@cloud9-sigitp2:~/environment/gv3-outposts-server-sjc38$ kubectl apply -f https://raw.githubusercontent.com/sigitp-git/outposts-server-eks-worker-node/main/sriov-device-plugin-ds-incl-arm64.yaml
serviceaccount/sriov-device-plugin created
daemonset.apps/kube-sriov-device-plugin-amd64 created
daemonset.apps/kube-sriov-device-plugin-ppc64le created
daemonset.apps/kube-sriov-device-plugin-arm64 created
ubuntu@cloud9-sigitp2:~/environment/gv3-outposts-server-sjc38$ kubectl get po -A
NAMESPACE                  NAME                                                              READY   STATUS    RESTARTS       AGE
amazon-cloudwatch          amazon-cloudwatch-observability-controller-manager-77474b58tq8t   1/1     Running   0              3d18h
amazon-cloudwatch          cloudwatch-agent-vdznr                                            1/1     Running   1 (3d1h ago)   3d1h
amazon-cloudwatch          fluent-bit-7d7p5                                                  1/1     Running   0              3d1h
external-dns               external-dns-5f6b4b64c9-gzpwc                                     1/1     Running   0              3d18h
kube-state-metrics         kube-state-metrics-5cc79c46bb-s4k4s                               1/1     Running   0              3d18h
kube-system                aws-node-czm6j                                                    2/2     Running   0              3d1h
kube-system                coredns-6b9575c64c-n7w57                                          1/1     Running   0              3d18h
kube-system                coredns-6b9575c64c-t468s                                          1/1     Running   0              3d18h
kube-system                eks-node-monitoring-agent-bl8wx                                   1/1     Running   0              3d1h
kube-system                eks-pod-identity-agent-r4cpz                                      1/1     Running   0              3d1h
kube-system                kube-proxy-hjqr7                                                  1/1     Running   0              3d1h
kube-system                kube-sriov-device-plugin-arm64-44g2p                              1/1     Running   0              6s
kube-system                metrics-server-6d67d68f67-4lgnv                                   1/1     Running   0              3d18h
kube-system                metrics-server-6d67d68f67-79qfv                                   1/1     Running   0              3d18h
monitoring                 sriov-metrics-exporter-2zgb4                                      1/1     Running   0              3d1h
prometheus-node-exporter   prometheus-node-exporter-6295l                                    1/1     Running   0              3d1h

## VF plumbed for the node
ubuntu@cloud9-sigitp2:~/environment/gv3-outposts-server-sjc38$ kubectl describe node ip-10-0-127-239.ec2.internal
Name:               ip-10-0-127-239.ec2.internal
Roles:              <none>
Labels:             beta.kubernetes.io/arch=arm64
                    beta.kubernetes.io/instance-type=ran1gd.metal
                    beta.kubernetes.io/os=linux
                    failure-domain.beta.kubernetes.io/region=us-east-1
                    failure-domain.beta.kubernetes.io/zone=us-east-1a
                    feature.node.kubernetes.io/network-sriov.capable=true
                    is_worker=true
                    k8s.io/cloud-provider-aws=a86e96846d12987a40a3489e34fc1703
                    kubernetes.io/arch=arm64
                    kubernetes.io/hostname=ip-10-0-127-239.ec2.internal
                    kubernetes.io/os=linux
                    node.kubernetes.io/instance-type=ran1gd.metal
                    node.longhorn.io/create-default-disk=true
                    storage=longhorn
                    topology.k8s.aws/zone-id=use1-az6
                    topology.kubernetes.io/region=us-east-1
                    topology.kubernetes.io/zone=us-east-1a
Annotations:        alpha.kubernetes.io/provided-node-ip: 10.0.127.239
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 14 May 2025 16:45:42 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  ip-10-0-127-239.ec2.internal
  AcquireTime:     <unset>
  RenewTime:       Sat, 17 May 2025 17:54:00 +0000
Conditions:
  Type                    Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----                    ------  -----------------                 ------------------                ------                       -------
  MemoryPressure          False   Sat, 17 May 2025 17:50:01 +0000   Wed, 14 May 2025 16:45:42 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure            False   Sat, 17 May 2025 17:50:01 +0000   Wed, 14 May 2025 16:45:42 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure             False   Sat, 17 May 2025 17:50:01 +0000   Wed, 14 May 2025 16:45:42 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready                   True    Sat, 17 May 2025 17:50:01 +0000   Wed, 14 May 2025 16:46:23 +0000   KubeletReady                 kubelet is posting ready status
  ContainerRuntimeReady   True    Thu, 15 May 2025 03:06:08 +0000   Wed, 14 May 2025 16:46:08 +0000   ContainerRuntimeIsReady      Monitoring for the ContainerRuntime system is active
  StorageReady            True    Thu, 15 May 2025 03:06:08 +0000   Wed, 14 May 2025 16:46:08 +0000   DiskIsReady                  Monitoring for the Disk system is active
  NetworkingReady         False   Thu, 15 May 2025 03:06:08 +0000   Wed, 14 May 2025 17:11:08 +0000   InterfaceNotUp               Interface "enp5s0f1np1" is not up
  KernelReady             True    Thu, 15 May 2025 03:06:08 +0000   Wed, 14 May 2025 16:46:08 +0000   KernelIsReady                Monitoring for the Kernel system is active
Addresses:
  InternalIP:   10.0.127.239
  ExternalIP:   184.72.82.201
  InternalDNS:  ip-10-0-127-239.ec2.internal
  Hostname:     ip-10-0-127-239.ec2.internal
  ExternalDNS:  ec2-184-72-82-201.compute-1.amazonaws.com
Capacity:
  amazon.com/bmn-mlx-sriov-pf1:  8
  amazon.com/bmn-mlx-sriov-pf2:  8
  cpu:                           64
  ephemeral-storage:             908274368Ki
  hugepages-1Gi:                 0
  hugepages-2Mi:                 0
  hugepages-32Mi:                0
  hugepages-64Ki:                0
  memory:                        263378616Ki
  pods:                          737
Allocatable:
  amazon.com/bmn-mlx-sriov-pf1:  8
  amazon.com/bmn-mlx-sriov-pf2:  8
  cpu:                           63770m
  ephemeral-storage:             835991914339
  hugepages-1Gi:                 0
  hugepages-2Mi:                 0
  hugepages-32Mi:                0
  hugepages-64Ki:                0
  memory:                        254713528Ki
  pods:                          737
System Info:
  Machine ID:                 e3e8fcf974444c51b67ec126263df8ae
  System UUID:                ec290dc0-2312-af71-5555-b73f025f3b05
  Boot ID:                    67a22448-1d83-4940-bc4c-5fdb7780a3c9
  Kernel Version:             6.8.0-1027-aws
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               arm64
  Container Runtime Version:  containerd://1.7.24
  Kubelet Version:            v1.32.3
  Kube-Proxy Version:         v1.32.3
ProviderID:                   aws:///us-east-1a/i-09347d58bcbd243b5
Non-terminated Pods:          (16 in total)
  Namespace                   Name                                                               CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                                               ------------  ----------  ---------------  -------------  ---
  amazon-cloudwatch           amazon-cloudwatch-observability-controller-manager-77474b58tq8t    100m (0%)     0 (0%)      64Mi (0%)        0 (0%)         3d19h
  amazon-cloudwatch           cloudwatch-agent-vdznr                                             250m (0%)     500m (0%)   128Mi (0%)       512Mi (0%)     3d1h
  amazon-cloudwatch           fluent-bit-7d7p5                                                   50m (0%)      500m (0%)   25Mi (0%)        250Mi (0%)     3d1h
  external-dns                external-dns-5f6b4b64c9-gzpwc                                      10m (0%)      0 (0%)      32Mi (0%)        64Mi (0%)      3d19h
  kube-state-metrics          kube-state-metrics-5cc79c46bb-s4k4s                                100m (0%)     0 (0%)      250Mi (0%)       400Mi (0%)     3d19h
  kube-system                 aws-node-czm6j                                                     50m (0%)      0 (0%)      0 (0%)           0 (0%)         3d1h
  kube-system                 coredns-6b9575c64c-n7w57                                           100m (0%)     0 (0%)      70Mi (0%)        170Mi (0%)     3d19h
  kube-system                 coredns-6b9575c64c-t468s                                           100m (0%)     0 (0%)      70Mi (0%)        170Mi (0%)     3d19h
  kube-system                 eks-node-monitoring-agent-bl8wx                                    10m (0%)      250m (0%)   30Mi (0%)        100Mi (0%)     3d1h
  kube-system                 eks-pod-identity-agent-r4cpz                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         3d1h
  kube-system                 kube-proxy-hjqr7                                                   100m (0%)     0 (0%)      0 (0%)           0 (0%)         3d1h
  kube-system                 kube-sriov-device-plugin-arm64-44g2p                               250m (0%)     1 (1%)      40Mi (0%)        200Mi (0%)     4m37s
  kube-system                 metrics-server-6d67d68f67-4lgnv                                    100m (0%)     0 (0%)      200Mi (0%)       400Mi (0%)     3d19h
  kube-system                 metrics-server-6d67d68f67-79qfv                                    100m (0%)     0 (0%)      200Mi (0%)       400Mi (0%)     3d19h
  monitoring                  sriov-metrics-exporter-2zgb4                                       100m (0%)     100m (0%)   100Mi (0%)       100Mi (0%)     3d1h
  prometheus-node-exporter    prometheus-node-exporter-6295l                                     100m (0%)     0 (0%)      30Mi (0%)        50Mi (0%)      3d1h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource                      Requests     Limits
  --------                      --------     ------
  cpu                           1520m (2%)   2350m (3%)
  memory                        1239Mi (0%)  2816Mi (1%)
  ephemeral-storage             0 (0%)       0 (0%)
  hugepages-1Gi                 0 (0%)       0 (0%)
  hugepages-2Mi                 0 (0%)       0 (0%)
  hugepages-32Mi                0 (0%)       0 (0%)
  hugepages-64Ki                0 (0%)       0 (0%)
  amazon.com/bmn-mlx-sriov-pf1  0            0
  amazon.com/bmn-mlx-sriov-pf2  0            0
Events:                         <none>
ubuntu@cloud9-sigitp2:~/environment/gv3-outposts-server-sjc38$ 


4. Deploy Multus daemonset thick client
The thick client deployment works on both AMD64 and ARM64

## AMD64
ubuntu@ip-10-0-10-242:~/environment/dpdk-testpmd-bmn-cluster$ kubectl apply -f https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/refs/heads/master/config/multus/v4.1.4-eksbuild.3/multus-daemonset-thick.yml
customresourcedefinition.apiextensions.k8s.io/network-attachment-definitions.k8s.cni.cncf.io created
clusterrole.rbac.authorization.k8s.io/multus created
clusterrolebinding.rbac.authorization.k8s.io/multus created
serviceaccount/multus created
configmap/multus-daemon-config created
daemonset.apps/kube-multus-ds created
ubuntu@ip-10-0-10-242:~/environment/dpdk-testpmd-bmn-cluster$
ubuntu@ip-10-0-10-242:~/environment/dpdk-testpmd-bmn-cluster$ kubectl get po -A
NAMESPACE                  NAME                                                              READY   STATUS    RESTARTS     AGE
amazon-cloudwatch          amazon-cloudwatch-observability-controller-manager-59c95f7dwspd   1/1     Running   1 (9h ago)   44h
amazon-cloudwatch          cloudwatch-agent-xnzf7                                            1/1     Running   1 (9h ago)   37h
amazon-cloudwatch          fluent-bit-qmqv5                                                  1/1     Running   1 (9h ago)   37h
external-dns               external-dns-7d4c9f944-nmq2j                                      1/1     Running   1 (9h ago)   44h
kube-state-metrics         kube-state-metrics-9c8dc6699-wmqxq                                1/1     Running   1 (9h ago)   44h
kube-system                aws-node-kqwvg                                                    2/2     Running   2 (9h ago)   37h
kube-system                coredns-6b9575c64c-fbnx6                                          1/1     Running   1 (9h ago)   44h
kube-system                coredns-6b9575c64c-x2shr                                          1/1     Running   1 (9h ago)   44h
kube-system                eks-node-monitoring-agent-c7r7q                                   1/1     Running   2 (9h ago)   37h
kube-system                eks-pod-identity-agent-8m4hh                                      1/1     Running   1 (9h ago)   37h
kube-system                kube-multus-ds-gr2dn                                              1/1     Running   0            65s
kube-system                kube-proxy-m6hxl                                                  1/1     Running   1 (9h ago)   37h
kube-system                kube-sriov-device-plugin-amd64-g6clx                              1/1     Running   0            9m37s
kube-system                metrics-server-6d67d68f67-tdftj                                   1/1     Running   1 (9h ago)   44h
kube-system                metrics-server-6d67d68f67-v7fb7                                   1/1     Running   1 (9h ago)   44h
monitoring                 sriov-metrics-exporter-hfkxd                                      1/1     Running   1 (9h ago)   32h
prometheus-node-exporter   prometheus-node-exporter-tcw6x                                    1/1     Running   1 (9h ago)   37h
ubuntu@ip-10-0-10-242:~/environment/dpdk-testpmd-bmn-cluster$

## ARM64
ubuntu@cloud9-sigitp2:~/environment/gv3-outposts-server-sjc38$ kubectl apply -f https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/refs/heads/master/config/multus/v4.1.4-eksbuild.3/multus-daemonset-thick.yml
customresourcedefinition.apiextensions.k8s.io/network-attachment-definitions.k8s.cni.cncf.io created
clusterrole.rbac.authorization.k8s.io/multus created
clusterrolebinding.rbac.authorization.k8s.io/multus created
serviceaccount/multus created
configmap/multus-daemon-config created
daemonset.apps/kube-multus-ds created

ubuntu@cloud9-sigitp2:~/environment/gv3-outposts-server-sjc38$ kubectl get po -A
NAMESPACE                  NAME                                                              READY   STATUS     RESTARTS       AGE
amazon-cloudwatch          amazon-cloudwatch-observability-controller-manager-77474b58tq8t   1/1     Running    0              3d19h
amazon-cloudwatch          cloudwatch-agent-vdznr                                            1/1     Running    1 (3d1h ago)   3d1h
amazon-cloudwatch          fluent-bit-7d7p5                                                  1/1     Running    0              3d1h
external-dns               external-dns-5f6b4b64c9-gzpwc                                     1/1     Running    0              3d19h
kube-state-metrics         kube-state-metrics-5cc79c46bb-s4k4s                               1/1     Running    0              3d19h
kube-system                aws-node-czm6j                                                    2/2     Running    0              3d1h
kube-system                coredns-6b9575c64c-n7w57                                          1/1     Running    0              3d19h
kube-system                coredns-6b9575c64c-t468s                                          1/1     Running    0              3d19h
kube-system                eks-node-monitoring-agent-bl8wx                                   1/1     Running    0              3d1h
kube-system                eks-pod-identity-agent-r4cpz                                      1/1     Running    0              3d1h
kube-system                kube-multus-ds-6rrtm                                              0/1     Init:0/1   0              22s
kube-system                kube-proxy-hjqr7                                                  1/1     Running    0              3d1h
kube-system                kube-sriov-device-plugin-arm64-44g2p                              1/1     Running    0              7m40s
kube-system                metrics-server-6d67d68f67-4lgnv                                   1/1     Running    0              3d19h
kube-system                metrics-server-6d67d68f67-79qfv                                   1/1     Running    0              3d19h
monitoring                 sriov-metrics-exporter-2zgb4                                      1/1     Running    0              3d1h
prometheus-node-exporter   prometheus-node-exporter-6295l                                    1/1     Running    0              3d1h


ubuntu@cloud9-sigitp2:~/environment/gv3-outposts-server-sjc38$ kubectl get po -A
NAMESPACE                  NAME                                                              READY   STATUS    RESTARTS       AGE
amazon-cloudwatch          amazon-cloudwatch-observability-controller-manager-77474b58tq8t   1/1     Running   0              3d19h
amazon-cloudwatch          cloudwatch-agent-vdznr                                            1/1     Running   1 (3d1h ago)   3d1h
amazon-cloudwatch          fluent-bit-7d7p5                                                  1/1     Running   0              3d1h
external-dns               external-dns-5f6b4b64c9-gzpwc                                     1/1     Running   0              3d19h
kube-state-metrics         kube-state-metrics-5cc79c46bb-s4k4s                               1/1     Running   0              3d19h
kube-system                aws-node-czm6j                                                    2/2     Running   0              3d1h
kube-system                coredns-6b9575c64c-n7w57                                          1/1     Running   0              3d19h
kube-system                coredns-6b9575c64c-t468s                                          1/1     Running   0              3d19h
kube-system                eks-node-monitoring-agent-bl8wx                                   1/1     Running   0              3d1h
kube-system                eks-pod-identity-agent-r4cpz                                      1/1     Running   0              3d1h
kube-system                kube-multus-ds-6rrtm                                              1/1     Running   0              66s
kube-system                kube-proxy-hjqr7                                                  1/1     Running   0              3d1h
kube-system                kube-sriov-device-plugin-arm64-44g2p                              1/1     Running   0              8m24s
kube-system                metrics-server-6d67d68f67-4lgnv                                   1/1     Running   0              3d19h
kube-system                metrics-server-6d67d68f67-79qfv                                   1/1     Running   0              3d19h
monitoring                 sriov-metrics-exporter-2zgb4                                      1/1     Running   0              3d1h
prometheus-node-exporter   prometheus-node-exporter-6295l                                    1/1     Running   0              3d1h
ubuntu@cloud9-sigitp2:~/environment/gv3-outposts-server-sjc38$ 



5. Deploy SRIOV-CNI daemonset
SRIOV CNI support both AMD64 and ARM64:
https://github.com/k8snetworkplumbingwg/sriov-cni/pkgs/container/sriov-cni, select OS ARM64
https://github.com/k8snetworkplumbingwg/sriov-cni/pkgs/container/sriov-cni/versions

## AMD64
ubuntu@ip-10-0-10-242:~/environment/dpdk-testpmd-bmn-cluster$ kubectl apply -f https://raw.githubusercontent.com/k8snetworkplumbingwg/sriov-cni/master/images/sriov-cni-daemonset.yaml
daemonset.apps/kube-sriov-cni-ds created
ubuntu@ip-10-0-10-242:~/environment/dpdk-testpmd-bmn-cluster$
ubuntu@ip-10-0-10-242:~/environment/dpdk-testpmd-bmn-cluster$ kubectl get po -A
NAMESPACE                  NAME                                                              READY   STATUS    RESTARTS     AGE
amazon-cloudwatch          amazon-cloudwatch-observability-controller-manager-59c95f7dwspd   1/1     Running   1 (9h ago)   44h
amazon-cloudwatch          cloudwatch-agent-xnzf7                                            1/1     Running   1 (9h ago)   37h
amazon-cloudwatch          fluent-bit-qmqv5                                                  1/1     Running   1 (9h ago)   37h
external-dns               external-dns-7d4c9f944-nmq2j                                      1/1     Running   1 (9h ago)   44h
kube-state-metrics         kube-state-metrics-9c8dc6699-wmqxq                                1/1     Running   1 (9h ago)   44h
kube-system                aws-node-kqwvg                                                    2/2     Running   2 (9h ago)   37h
kube-system                coredns-6b9575c64c-fbnx6                                          1/1     Running   1 (9h ago)   44h
kube-system                coredns-6b9575c64c-x2shr                                          1/1     Running   1 (9h ago)   44h
kube-system                eks-node-monitoring-agent-c7r7q                                   1/1     Running   2 (9h ago)   37h
kube-system                eks-pod-identity-agent-8m4hh                                      1/1     Running   1 (9h ago)   37h
kube-system                kube-multus-ds-gr2dn                                              1/1     Running   0            2m27s
kube-system                kube-proxy-m6hxl                                                  1/1     Running   1 (9h ago)   37h
kube-system                kube-sriov-cni-ds-d75mg                                           1/1     Running   0            17s
kube-system                kube-sriov-device-plugin-amd64-g6clx                              1/1     Running   0            10m
kube-system                metrics-server-6d67d68f67-tdftj                                   1/1     Running   1 (9h ago)   44h
kube-system                metrics-server-6d67d68f67-v7fb7                                   1/1     Running   1 (9h ago)   44h
monitoring                 sriov-metrics-exporter-hfkxd                                      1/1     Running   1 (9h ago)   32h
prometheus-node-exporter   prometheus-node-exporter-tcw6x                                    1/1     Running   1 (9h ago)   37h
ubuntu@ip-10-0-10-242:~/environment/dpdk-testpmd-bmn-cluster$

## SRIOV CNI binary inside the host
[root@ip-10-0-58-16 bin]# ls -la
total 225688
drwxr-xr-x. 2 root root    16384 May  9 03:56 .
drwxr-xr-x. 3 root root       17 May  1 01:13 ..
-rw-r--r--. 1 root root    11357 May  8 18:40 LICENSE
-rw-r--r--. 1 root root     2343 May  8 18:40 README.md
-rwxr-xr-x. 1 root root 20304193 May  8 18:40 aws-cni
-rwxr-xr-x. 1 root root    32354 May  8 18:40 aws-cni-support.sh
-rwxr-xr-x. 1 root root  4272898 May  8 18:40 bandwidth
-rwxr-xr-x. 1 root root  4788191 May  8 18:40 bridge
-rwxr-xr-x. 1 root root 11419738 May  8 18:40 dhcp
-rwxr-xr-x. 1 root root  4424930 May  8 18:40 dummy
-rwxr-xr-x. 1 root root 11731265 May  8 18:40 egress-cni
-rwxr-xr-x. 1 root root  4943846 May  8 18:40 firewall
-rwxr-xr-x. 1 root root  4345300 May  8 18:40 host-device
-rwxr-xr-x. 1 root root  3679575 May  8 18:40 host-local
-rwxr-xr-x. 1 root root  4443729 May  8 18:40 ipvlan
-rwxr-xr-x. 1 root root  3750882 May  8 18:40 loopback
-rwxr-xr-x. 1 root root  4480422 May  8 18:40 macvlan
-rwxr-xr-x. 1 root root 48218431 May  9 03:42 multus-shim
-rwxr-xr-x. 1 root root  4228332 May  8 18:40 portmap
-rwxr-xr-x. 1 root root  4602833 May  8 18:40 ptp
-rwxr-xr-x. 1 root root  3957166 May  8 18:40 sbr
-rwxr-xr-x. 1 root root  5006876 May  9 03:45 sriov
-rwxr-xr-x. 1 root root  3223947 May  8 18:40 static
-rwxr-xr-x. 1 root root  4503742 May  8 18:40 tap
-rwxr-xr-x. 1 root root  3838043 May  8 18:40 tuning
-rwxr-xr-x. 1 root root  4440528 May  8 18:40 vlan
-rwxr-xr-x. 1 root root  4103500 May  8 18:40 vrf
-rwxr-xr-x. 1 root root 62276195 May  9 03:56 whereabouts
[root@ip-10-0-58-16 bin]# 

## ARM64
https://github.com/k8snetworkplumbingwg/sriov-cni/pkgs/container/sriov-cni, select OS ARM64
docker pull ghcr.io/k8snetworkplumbingwg/sriov-cni:3271b0f594b6d814f2e83a43589d99c6bd1882f3@sha256:cfca98a15f33f0e045849630c4c74d605ae64f41ca6f94efe56f6bcb291759e2

ubuntu@cloud9-sigitp2:~/environment/gv3-outposts-server-sjc38$ docker pull ghcr.io/k8snetworkplumbingwg/sriov-cni:3271b0f594b6d814f2e83a43589d99c6bd1882f3@sha256:cfca98a15f33f0e045849630c4c74d605ae64f41ca6f94efe56f6bcb291759e2
ghcr.io/k8snetworkplumbingwg/sriov-cni@sha256:cfca98a15f33f0e045849630c4c74d605ae64f41ca6f94efe56f6bcb291759e2: Pulling from k8snetworkplumbingwg/sriov-cni
6e771e15690e: Pull complete 
ade896a11b43: Pull complete 
48a6bed01cc9: Pull complete 
Digest: sha256:cfca98a15f33f0e045849630c4c74d605ae64f41ca6f94efe56f6bcb291759e2
Status: Downloaded newer image for ghcr.io/k8snetworkplumbingwg/sriov-cni@sha256:cfca98a15f33f0e045849630c4c74d605ae64f41ca6f94efe56f6bcb291759e2
ghcr.io/k8snetworkplumbingwg/sriov-cni:3271b0f594b6d814f2e83a43589d99c6bd1882f3@sha256:cfca98a15f33f0e045849630c4c74d605ae64f41ca6f94efe56f6bcb291759e2

ubuntu@cloud9-sigitp2:~/environment/gv3-outposts-server-sjc38$ docker images                                                                              
REPOSITORY                                                         TAG                      IMAGE ID       CREATED        SIZE
ghcr.io/k8snetworkplumbingwg/sriov-cni                             <none>                   93a1a68cbe23   10 days ago    13.1MB
ghcr.io/k8snetworkplumbingwg/sriov-cni                             latest-amd64             a09549570698   3 months ago   12.8MB
291615555612.dkr.ecr.us-east-1.amazonaws.com/sigitp-ecr            ubuntu-mlnx-dpdk-amd64   87e987b1aaa3   8 months ago   1.37GB
public.ecr.aws/h8q5n8w4/sigitp-ecr-public/ubuntu-mlnx-dpdk-amd64   latest                   87e987b1aaa3   8 months ago   1.37GB
ubuntu@cloud9-sigitp2:~/environment/gv3-outposts-server-sjc38$ 

ubuntu@cloud9-sigitp2:~/environment/gv3-outposts-server-sjc38$ docker images                                                                              

REPOSITORY                                                         TAG                      IMAGE ID       CREATED        SIZE
ghcr.io/k8snetworkplumbingwg/sriov-cni                             <none>                   93a1a68cbe23   10 days ago    13.1MB
ghcr.io/k8snetworkplumbingwg/sriov-cni                             latest-amd64             a09549570698   3 months ago   12.8MB
291615555612.dkr.ecr.us-east-1.amazonaws.com/sigitp-ecr            ubuntu-mlnx-dpdk-amd64   87e987b1aaa3   8 months ago   1.37GB
public.ecr.aws/h8q5n8w4/sigitp-ecr-public/ubuntu-mlnx-dpdk-amd64   latest                   87e987b1aaa3   8 months ago   1.37GB
ubuntu@cloud9-sigitp2:~/environment/gv3-outposts-server-sjc38$ 


ubuntu@cloud9-sigitp2:~/environment/gv3-outposts-server-sjc38$ docker tag 93a1a68cbe23 ghcr.io/k8snetworkplumbingwg/sriov-cni:latest-arm64
ubuntu@cloud9-sigitp2:~/environment/gv3-outposts-server-sjc38$ docker images
REPOSITORY                                                         TAG                      IMAGE ID       CREATED        SIZE
ghcr.io/k8snetworkplumbingwg/sriov-cni                             latest-arm64             93a1a68cbe23   10 days ago    13.1MB
ghcr.io/k8snetworkplumbingwg/sriov-cni                             latest-amd64             a09549570698   3 months ago   12.8MB
public.ecr.aws/h8q5n8w4/sigitp-ecr-public/ubuntu-mlnx-dpdk-amd64   latest                   87e987b1aaa3   8 months ago   1.37GB
291615555612.dkr.ecr.us-east-1.amazonaws.com/sigitp-ecr            ubuntu-mlnx-dpdk-amd64   87e987b1aaa3   8 months ago   1.37GB
ubuntu@cloud9-sigitp2:~/environment/gv3-outposts-server-sjc38$ 

## still not successful 

ubuntu@cloud9-sigitp2:~/environment/gv3-outposts-server-sjc38$ kubectl apply -f https://raw.githubusercontent.com/sigitp-git/outposts-server-eks-worker-node/refs/heads/main/sriov-cni-daemonset-arm64.yaml
daemonset.apps/kube-sriov-cni-ds created


ubuntu@cloud9-sigitp2:~/environment/gv3-outposts-server-sjc38$ kubectl get po -A                                                                          
NAMESPACE                  NAME                                                              READY   STATUS              RESTARTS       AGE
amazon-cloudwatch          amazon-cloudwatch-observability-controller-manager-77474b58tq8t   1/1     Running             0              3d20h
amazon-cloudwatch          cloudwatch-agent-vdznr                                            1/1     Running             1 (3d2h ago)   3d2h
amazon-cloudwatch          fluent-bit-7d7p5                                                  1/1     Running             0              3d2h
external-dns               external-dns-5f6b4b64c9-gzpwc                                     1/1     Running             0              3d20h
kube-state-metrics         kube-state-metrics-5cc79c46bb-s4k4s                               1/1     Running             0              3d20h
kube-system                aws-node-czm6j                                                    2/2     Running             0              3d2h
kube-system                coredns-6b9575c64c-n7w57                                          1/1     Running             0              3d20h
kube-system                coredns-6b9575c64c-t468s                                          1/1     Running             0              3d20h
kube-system                eks-node-monitoring-agent-bl8wx                                   1/1     Running             0              3d2h
kube-system                eks-pod-identity-agent-r4cpz                                      1/1     Running             0              3d2h
kube-system                kube-multus-ds-6rrtm                                              1/1     Running             0              88m
kube-system                kube-proxy-hjqr7                                                  1/1     Running             0              3d2h
kube-system                kube-sriov-cni-ds-rz47v                                           0/1     ContainerCreating   0              5s
kube-system                kube-sriov-device-plugin-arm64-44g2p                              1/1     Running             0              96m
kube-system                metrics-server-6d67d68f67-4lgnv                                   1/1     Running             0              3d20h
kube-system                metrics-server-6d67d68f67-79qfv                                   1/1     Running             0              3d20h
monitoring                 sriov-metrics-exporter-2zgb4                                      1/1     Running             0              3d2h
prometheus-node-exporter   prometheus-node-exporter-6295l                                    1/1     Running             0              3d2h
ubuntu@cloud9-sigitp2:~/environment/gv3-outposts-server-sjc38$ 

## successful after restarting the vpc cni and the multus kube-multus-ds-6rrtm pods
## restart sequence: vpc-cni (aws-node), multus, sriov-cni
ubuntu@cloud9-sigitp2:~$ kubectl get po -A
NAMESPACE                  NAME                                                              READY   STATUS    RESTARTS       AGE
amazon-cloudwatch          amazon-cloudwatch-observability-controller-manager-77474b58tq8t   1/1     Running   0              6d
amazon-cloudwatch          cloudwatch-agent-vdznr                                            1/1     Running   1 (5d6h ago)   5d6h
amazon-cloudwatch          fluent-bit-7d7p5                                                  1/1     Running   0              5d6h
external-dns               external-dns-5f6b4b64c9-gzpwc                                     1/1     Running   0              6d
kube-state-metrics         kube-state-metrics-5cc79c46bb-s4k4s                               1/1     Running   0              6d
kube-system                aws-node-s572p                                                    2/2     Running   0              38s
kube-system                coredns-6b9575c64c-n7w57                                          1/1     Running   0              6d
kube-system                coredns-6b9575c64c-t468s                                          1/1     Running   0              6d
kube-system                eks-node-monitoring-agent-bl8wx                                   1/1     Running   0              5d6h
kube-system                eks-pod-identity-agent-r4cpz                                      1/1     Running   0              5d6h
kube-system                kube-multus-ds-llvwz                                              1/1     Running   0              28s
kube-system                kube-proxy-hjqr7                                                  1/1     Running   0              5d6h
kube-system                kube-sriov-cni-ds-p4rdq                                           1/1     Running   0              16s
kube-system                kube-sriov-device-plugin-arm64-44g2p                              1/1     Running   0              2d5h
kube-system                metrics-server-6d67d68f67-4lgnv                                   1/1     Running   0              6d
kube-system                metrics-server-6d67d68f67-79qfv                                   1/1     Running   0              6d
monitoring                 sriov-metrics-exporter-2zgb4                                      1/1     Running   0              5d6h
prometheus-node-exporter   prometheus-node-exporter-6295l                                    1/1     Running   0              5d6h
ubuntu@cloud9-sigitp2:~$

6. Deploy NAD CRDs
ubuntu@ip-10-0-10-242:~/environment/dpdk-testpmd-bmn-cluster$ kubectl apply -f nad-n3-1001-numa1p0-pf3.yaml
networkattachmentdefinition.k8s.cni.cncf.io/n3-1001-numa1p0-pf3 created
ubuntu@ip-10-0-10-242:~/environment/dpdk-testpmd-bmn-cluster$
ubuntu@ip-10-0-10-242:~/environment/dpdk-testpmd-bmn-cluster$ kubectl apply -f nad-n3-1002-numa1p1-pf4.yaml
networkattachmentdefinition.k8s.cni.cncf.io/n3-1002-numa1p1-pf4 created
ubuntu@ip-10-0-10-242:~/environment/dpdk-testpmd-bmn-cluster$
ubuntu@ip-10-0-10-242:~/environment/dpdk-testpmd-bmn-cluster$ kubectl get network-attachment-definition
NAME                  AGE
n3-1001-numa1p0-pf3   33s
n3-1002-numa1p1-pf4   11s
ubuntu@ip-10-0-10-242:~/environment/dpdk-testpmd-bmn-cluster$
7. Deploy Whereabouts
https://github.com/k8snetworkplumbingwg/whereabouts
ubuntu@ip-10-0-10-242:~/environment/dpdk-testpmd-bmn-cluster$ kubectl apply -f https://raw.githubusercontent.com/k8snetworkplumbingwg/whereabouts/refs/heads/master/doc/crds/daemonset-install.yaml
serviceaccount/whereabouts created
clusterrolebinding.rbac.authorization.k8s.io/whereabouts created
clusterrole.rbac.authorization.k8s.io/whereabouts-cni created
configmap/whereabouts-config created
daemonset.apps/whereabouts created
ubuntu@ip-10-0-10-242:~/environment/dpdk-testpmd-bmn-cluster$
ubuntu@ip-10-0-10-242:~/environment/dpdk-testpmd-bmn-cluster$ kubectl  apply -f https://raw.githubusercontent.com/k8snetworkplumbingwg/whereabouts/refs/heads/master/doc/crds/whereabouts.cni.cncf.io_ippools.yaml
customresourcedefinition.apiextensions.k8s.io/ippools.whereabouts.cni.cncf.io created
ubuntu@ip-10-0-10-242:~/environment/dpdk-testpmd-bmn-cluster$ kubectl  apply -f https://raw.githubusercontent.com/k8snetworkplumbingwg/whereabouts/refs/heads/master/doc/crds/whereabouts.cni.cncf.io_overlappingrangeipreservations.yaml
customresourcedefinition.apiextensions.k8s.io/overlappingrangeipreservations.whereabouts.cni.cncf.io created
ubuntu@ip-10-0-10-242:~/environment/dpdk-testpmd-bmn-cluster$
ubuntu@ip-10-0-10-242:~/environment/dpdk-testpmd-bmn-cluster$ kubectl get po -A
NAMESPACE                  NAME                                                              READY   STATUS    RESTARTS     AGE
amazon-cloudwatch          amazon-cloudwatch-observability-controller-manager-59c95f7dwspd   1/1     Running   1 (9h ago)   44h
amazon-cloudwatch          cloudwatch-agent-xnzf7                                            1/1     Running   1 (9h ago)   37h
amazon-cloudwatch          fluent-bit-qmqv5                                                  1/1     Running   1 (9h ago)   37h
external-dns               external-dns-7d4c9f944-nmq2j                                      1/1     Running   1 (9h ago)   44h
kube-state-metrics         kube-state-metrics-9c8dc6699-wmqxq                                1/1     Running   1 (9h ago)   44h
kube-system                aws-node-kqwvg                                                    2/2     Running   2 (9h ago)   37h
kube-system                coredns-6b9575c64c-fbnx6                                          1/1     Running   1 (9h ago)   44h
kube-system                coredns-6b9575c64c-x2shr                                          1/1     Running   1 (9h ago)   44h
kube-system                eks-node-monitoring-agent-c7r7q                                   1/1     Running   2 (9h ago)   37h
kube-system                eks-pod-identity-agent-8m4hh                                      1/1     Running   1 (9h ago)   37h
kube-system                kube-multus-ds-gr2dn                                              1/1     Running   0            21m
kube-system                kube-proxy-m6hxl                                                  1/1     Running   1 (9h ago)   37h
kube-system                kube-sriov-cni-ds-d75mg                                           1/1     Running   0            19m
kube-system                kube-sriov-device-plugin-amd64-g6clx                              1/1     Running   0            29m
kube-system                metrics-server-6d67d68f67-tdftj                                   1/1     Running   1 (9h ago)   44h
kube-system                metrics-server-6d67d68f67-v7fb7                                   1/1     Running   1 (9h ago)   44h
kube-system                whereabouts-l9955                                                 1/1     Running   0            7m19s
monitoring                 sriov-metrics-exporter-hfkxd                                      1/1     Running   1 (9h ago)   32h
prometheus-node-exporter   prometheus-node-exporter-tcw6x                                    1/1     Running   1 (9h ago)   37h
ubuntu@ip-10-0-10-242:~/environment/dpdk-testpmd-bmn-cluster$
8. Deploy DPDK Pods
ubuntu@ip-10-0-10-242:~/environment/dpdk-testpmd-bmn-cluster$ kubectl apply -f mlnx-dpdk-1001-1002-node1-tx.yaml
pod/mlnx-dpdk-1001-1002-node1-tx created
ubuntu@ip-10-0-10-242:~/environment/dpdk-testpmd-bmn-cluster$ kubectl apply -f mlnx-dpdk-1001-1002-node1-rx.yaml
pod/mlnx-dpdk-1001-1002-node1-rx created
ubuntu@ip-10-0-10-242:~/environment/dpdk-testpmd-bmn-cluster$
ubuntu@ip-10-0-10-242:~/environment/dpdk-testpmd-bmn-cluster$ kubectl get po -A
NAMESPACE                  NAME                                                              READY   STATUS    RESTARTS      AGE
amazon-cloudwatch          amazon-cloudwatch-observability-controller-manager-59c95f7dwspd   1/1     Running   1 (76m ago)   36h
amazon-cloudwatch          cloudwatch-agent-xnzf7                                            1/1     Running   1 (76m ago)   29h
amazon-cloudwatch          fluent-bit-qmqv5                                                  1/1     Running   1 (76m ago)   29h
default                    mlnx-dpdk-1001-1002-node1-rx                                      0/1     Pending   0             14s
default                    mlnx-dpdk-1001-1002-node1-tx                                      0/1     Pending   0             30s
external-dns               external-dns-7d4c9f944-nmq2j                                      1/1     Running   1 (76m ago)   36h
kube-state-metrics         kube-state-metrics-9c8dc6699-wmqxq                                1/1     Running   1 (76m ago)   35h
kube-system                aws-node-kqwvg                                                    2/2     Running   2 (76m ago)   29h
kube-system                coredns-6b9575c64c-fbnx6                                          1/1     Running   1 (60m ago)   36h
kube-system                coredns-6b9575c64c-x2shr                                          1/1     Running   1 (60m ago)   36h
kube-system                eks-node-monitoring-agent-c7r7q                                   1/1     Running   2 (59m ago)   29h
kube-system                eks-pod-identity-agent-8m4hh                                      1/1     Running   1 (76m ago)   29h
kube-system                kube-multus-ds-hxzqk                                              1/1     Running   0             7m10s
kube-system                kube-proxy-m6hxl                                                  1/1     Running   1 (76m ago)   29h
kube-system                kube-sriov-cni-ds-kjftl                                           1/1     Running   0             4m40s
kube-system                kube-sriov-device-plugin-amd64-mzmj4                              1/1     Running   0             8m52s
kube-system                metrics-server-6d67d68f67-tdftj                                   1/1     Running   1 (76m ago)   36h
kube-system                metrics-server-6d67d68f67-v7fb7                                   1/1     Running   1 (76m ago)   36h
monitoring                 sriov-metrics-exporter-hfkxd                                      1/1     Running   1 (76m ago)   24h
prometheus-node-exporter   prometheus-node-exporter-tcw6x                                    1/1     Running   1 (76m ago)   29h
ubuntu@ip-10-0-10-242:~/environment/dpdk-testpmd-bmn-cluster$
10. Configuration of AMP and AMG
11. Send and Receive Traffic with DPDK App
12. Grafana Dashboard of SRIOV Metrics
========================================================================================================================
DPDK TESTPMD APPLICATION CONFIGS
========================================================================================================================
1. DPDK-TX
======================================
ifconfig net1
ethtool -i net1
KUBEPOD_SLICE=$(cut -d: -f3 /proc/self/cgroup); cat /sys/fs/cgroup$KUBEPOD_SLICE/cpuset.cpus.effective
./build/app/dpdk-testpmd -l 48-64,144-159 -n 6 -a 0001:16:01.2,mprq_en=1,rxqs_min_mprq=1,mprq_log_stride_num=9,txq_inline_mpw=128,rxq_pkt_pad_en=1 --file-prefix sigitp-dpdk-test --socket-mem=4096,4096 --proc-type=auto -- --mbcache=512 --burst=64 --nb-cores=32 --rxq=24 --txq=24 -i --rxd=8192 --txd=8192 --forward-mode=txonly --txonly-multi-flow --tx-ip=169.30.1.2,169.30.1.3 --eth-peer=0,aa:64:62:7a:97:27
--------------------------------------
export KUBEPOD_SLICE=$(cut -d: -f3 /proc/self/cgroup); export CPU=$(cat /sys/fs/cgroup$KUBEPOD_SLICE/cpuset.cpus.effective)
echo ${CPU}
export PCI=$(ethtool -i net1 | grep bus-info | awk '{print $2}')
echo ${PCI}
export IP=$(ifconfig net1 | grep inet | awk '{print $2}')
echo ${IP}
./build/app/dpdk-testpmd -l ${CPU} -n 6 -a ${PCI},mprq_en=1,rxqs_min_mprq=1,mprq_log_stride_num=9,txq_inline_mpw=128,rxq_pkt_pad_en=1 --file-prefix sigitp-dpdk-test --socket-mem=4096,4096 --proc-type=auto -- --mbcache=512 --burst=64 --nb-cores=32 --rxq=24 --txq=24 -i --rxd=8192 --txd=8192 --forward-mode=txonly --txonly-multi-flow --tx-ip=${IP},169.30.1.5 --eth-peer=0,62:3e:f7:19:8e:93
#optional
lspci -v -nn -mm -k -s ${PCI}
--socket-mem=8192,0
2. DPDK-RX
======================================
ifconfig net1
ethtool -i net1
KUBEPOD_SLICE=$(cut -d: -f3 /proc/self/cgroup); cat /sys/fs/cgroup$KUBEPOD_SLICE/cpuset.cpus.effective
./build/app/dpdk-testpmd -l 65-80,160-176 -a 0001:16:00.7,mprq_en=1,rxqs_min_mprq=1,mprq_log_stride_num=9,txq_inline_mpw=128,rxq_pkt_pad_en=1 --file-prefix sigitp-dpdk-test -- --nb-cores=32 --rxq=24 --txq=24 -i --forward-mode=rxonly --eth-peer=0,2e:61:54:1c:3d:73 --stats-period 5
--------------------------------------
export KUBEPOD_SLICE=$(cut -d: -f3 /proc/self/cgroup); export CPU=$(cat /sys/fs/cgroup$KUBEPOD_SLICE/cpuset.cpus.effective)
echo ${CPU}
export PCI=$(ethtool -i net1 | grep bus-info | awk '{print $2}')
echo ${PCI}
export IP=$(ifconfig net1 | grep inet | awk '{print $2}')
echo ${IP}
./build/app/dpdk-testpmd -l ${CPU} -a ${PCI},mprq_en=1,rxqs_min_mprq=1,mprq_log_stride_num=9,txq_inline_mpw=128,rxq_pkt_pad_en=1 --file-prefix sigitp-dpdk-test -- --nb-cores=32 --rxq=24 --txq=24 -i --forward-mode=rxonly --eth-peer=0,12:63:7e:96:46:82 --stats-period 5
#optional
lspci -v -nn -mm -k -s ${PCI}
--socket-mem=8192,0
3. Modifying Huge Pages Number
======================================
grep Hugepagesize /proc/meminfo
echo 32 > /proc/sys/vm/nr_hugepages or sysctl -w vm.nr_hugepages=32
permanent: echo "vm.nr_hugepages=512" >> /etc/sysctl.conf
grep HugePages_Total /proc/meminfo
grep HugePages_Free /proc/meminfo
grep MemFree /proc/meminfo
service kubelet restart
service kubelet status
--initial setup with restart required--
sudo sed -i 's/selinux=1/& default_hugepagesz=1GB hugepagesz=1G hugepages=32/g' /etc/default/grub
sudo grub2-mkconfig -o /boot/grub2/grub.cfg
4. Adding VF to PF
======================================
vim /bin/create-virtual-function.sh
change VF value to 0 before changing to new value (16, 32, more, depends on firmware version)
/bin/create-virtual-function.sh
5. SRIOV Metrics Exporter, worker node reinstall
======================================
helm uninstall prometheus-for-amp prometheus-community/prometheus -n prometheus
kubectl delete pvc storage-prometheus-for-amp-alertmanager-0 -n prometheus
kubectl delete pvc storage-volume-prometheus-for-amp-server-0 -n prometheus
    #note that pv and pvc created automatically by helm chart utilizing storageclass
IAM_PROXY_PROMETHEUS_ROLE_ARN=arn:aws:iam::291615555612:role/EKS-AMP-ServiceAccount-Role
WORKSPACE_ID=ws-23c82b83-eaeb-480f-b9e8-c2e788025465
AWS_REGION=us-east-1
helm install prometheus-for-amp prometheus-community/prometheus -n prometheus -f ./amp_ingest_override_values-storageclass-modify.yaml \--set serviceAccounts.server.annotations."eks\.amazonaws\.com/role-arn"="${IAM_PROXY_PROMETHEUS_ROLE_ARN}" \--set server.remoteWrite[0].url="https://aps-workspaces.${AWS_REGION}.amazonaws.com/workspaces/${WORKSPACE_ID}/api/v1/remote_write" \--set server.remoteWrite[0].sigv4.region=${AWS_REGION}
#amp-config-map
kubectl apply -f cm-prometheus-for-amp-server-n-prometheus-with-sriov-DEV1.yaml --force
#sriov-metrics-exporter
kubectl label node ip-10-0-62-173.ec2.internal feature.node.kubernetes.io/network-sriov.capable="true"
6. HugePages Modification for EKS Worker Node
======================================
sh-5.2$ grep Hugepagesize /proc/meminfo
Hugepagesize:    1048576 kB
sh-5.2$ grep HugePages_Total /proc/meminfo
HugePages_Total:       8
sh-5.2$
sh-5.2$ cat /proc/sys/vm/nr_hugepages
8
sh-5.2$
## MODIFICATION OF HUGEPAGES NUMBER, NO RESTART NEEDED
[root@ip-10-0-48-74 bin]# sysctl -w vm.nr_hugepages=32
vm.nr_hugepages = 32
[root@ip-10-0-48-74 bin]#
[root@ip-10-0-48-74 bin]# echo "vm.nr_hugepages=32" >> /etc/sysctl.conf
[root@ip-10-0-48-74 bin]# cat /etc/sysctl.conf
fs.inotify.max_user_watches=524288
fs.inotify.max_user_instances=8192
vm.max_map_count=524288
kernel.pid_max=4194304
vm.nr_hugepages=32
[root@ip-10-0-48-74 bin]#
[root@ip-10-0-48-74 bin]# grep HugePages_Total /proc/meminfo
HugePages_Total:      32
[root@ip-10-0-48-74 bin]#
[root@ip-10-0-48-74 bin]# grep HugePages_Free /proc/meminfo
HugePages_Free:       24
[root@ip-10-0-48-74 bin]#
[root@ip-10-0-48-74 bin]# grep MemFree /proc/meminfo
MemFree:        1012834412 kB
[root@ip-10-0-48-74 bin]#
## RESTART KUBELET ON WORKER NODE
[root@ip-10-0-48-74 ~]# systemctl status kubelet
 kubelet.service - Kubernetes Kubelet
     Loaded: loaded (/etc/systemd/system/kubelet.service; disabled; preset: disabled)
     Active: active (running) since Wed 2025-01-08 21:45:33 UTC; 1 day 21h ago
       Docs: https://github.com/kubernetes/kubernetes
   Main PID: 18560 (kubelet)
      Tasks: 112 (limit: 629145)
     Memory: 177.3M
        CPU: 1h 30min 22.144s
     CGroup: /runtime.slice/kubelet.service
             18560 /usr/bin/kubelet --cloud-provider=external --hostname-override=ip-10-0-48-74.ec2.internal --config=/etc/kubernetes/kubelet/config.json --config-dir=/etc/kubernetes/kubelet/config.json.d --kubeconfig=/var/lib/kubele>
Jan 10 16:59:27 ip-10-0-48-74.ec2.internal kubelet[18560]: I0110 16:59:27.123028   18560 state_mem.go:80] "Updated desired CPUSet" podUID="f89180c9-7d48-4e64-860f-03cf04c25ad9" containerName="instance-manager" cpuSet="0-47,65-143,160-1>
Jan 10 16:59:27 ip-10-0-48-74.ec2.internal kubelet[18560]: I0110 16:59:27.130856   18560 state_mem.go:80] "Updated desired CPUSet" podUID="836e359d-9757-4232-84b3-c44cfa669547" containerName="prometheus-server-configmap-reload" cpuSet=>
Jan 10 16:59:27 ip-10-0-48-74.ec2.internal kubelet[18560]: I0110 16:59:27.131667   18560 kubelet.go:2448] "SyncLoop UPDATE" source="api" pods=["default/mlnx-dpdk-1001-1002-node2-tx"]
Jan 10 16:59:27 ip-10-0-48-74.ec2.internal kubelet[18560]: I0110 16:59:27.139799   18560 state_mem.go:80] "Updated desired CPUSet" podUID="836e359d-9757-4232-84b3-c44cfa669547" containerName="prometheus-server" cpuSet="0-47,65-143,160->
Jan 10 16:59:27 ip-10-0-48-74.ec2.internal kubelet[18560]: I0110 16:59:27.262353   18560 state_mem.go:80] "Updated desired CPUSet" podUID="7ba44264-323d-4caa-87ec-fb1c69608d8d" containerName="node-exporter" cpuSet="0-47,65-143,160-191"
Jan 10 16:59:27 ip-10-0-48-74.ec2.internal kubelet[18560]: I0110 16:59:27.292468   18560 state_mem.go:80] "Updated desired CPUSet" podUID="e75cf515-125f-47c1-9e89-70bd3e8aa4e1" containerName="ubuntu-mlnx-dpdk" cpuSet="48-64,144-159"
Jan 10 16:59:28 ip-10-0-48-74.ec2.internal kubelet[18560]: I0110 16:59:28.265230   18560 kubelet.go:2473] "SyncLoop (PLEG): event for pod" pod="default/mlnx-dpdk-1001-1002-node2-tx" event={"ID":"e75cf515-125f-47c1-9e89-70bd3e8aa4e1","T>
Jan 10 16:59:28 ip-10-0-48-74.ec2.internal kubelet[18560]: I0110 16:59:28.265256   18560 kubelet.go:2473] "SyncLoop (PLEG): event for pod" pod="default/mlnx-dpdk-1001-1002-node2-tx" event={"ID":"e75cf515-125f-47c1-9e89-70bd3e8aa4e1","T>
Jan 10 16:59:28 ip-10-0-48-74.ec2.internal kubelet[18560]: I0110 16:59:28.424377   18560 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/mlnx-dpdk-1001-1002-node2-tx" podStartSLOduration=7.424367882 pod>
Jan 10 18:51:43 ip-10-0-48-74.ec2.internal kubelet[18560]: E0110 18:51:43.340831   18560 server.go:917] query parameter "port" is required
[root@ip-10-0-48-74 ~]# ps ax | grep kubelet
  18560 ?        Ssl   88:59 /usr/bin/kubelet --cloud-provider=external --hostname-override=ip-10-0-48-74.ec2.internal --config=/etc/kubernetes/kubelet/config.json --config-dir=/etc/kubernetes/kubelet/config.json.d --kubeconfig=/var/lib/kubelet/kubeconfig --image-credential-provider-bin-dir=/etc/eks/image-credential-provider --image-credential-provider-config=/etc/eks/image-credential-provider/config.json --node-ip=10.0.48.74 --node-labels=node.longhorn.io/create-default-disk=true,storage=longhorn,is_worker=true --topology-manager-policy=single-numa-node --cpu-manager-policy=static
  22029 ?        Ssl    0:37 sriov-exporter --path.kubecgroup=/host/kubecgroup --path.sysbuspci=/host/sys/bus/pci/devices/ --path.sysclassnet=/host/sys/class/net/ --path.cpucheckpoint=/host/cpu_manager_state --path.kubeletsocket=/host/kubelet.sock --collector.kubepoddevice=true --collector.vfstatspriority=sysfs,netlink
 103015 ?        Ssl    0:05 /csi-node-driver-registrar --v=2 --csi-address=/csi/csi.sock --kubelet-registration-path=/var/lib/kubelet/plugins/driver.longhorn.io/csi.sock
3534266 pts/1    S+     0:00 grep --color=auto kubelet
[root@ip-10-0-48-74 ~]# systemctl restart kubelet
[root@ip-10-0-48-74 ~]# systemctl status kubelet
 kubelet.service - Kubernetes Kubelet
     Loaded: loaded (/etc/systemd/system/kubelet.service; disabled; preset: disabled)
     Active: active (running) since Fri 2025-01-10 19:11:19 UTC; 3s ago
       Docs: https://github.com/kubernetes/kubernetes
    Process: 3535355 ExecStartPre=/sbin/iptables -P FORWARD ACCEPT -w 5 (code=exited, status=0/SUCCESS)
   Main PID: 3535357 (kubelet)
      Tasks: 32 (limit: 629145)
     Memory: 51.5M
        CPU: 1.165s
     CGroup: /runtime.slice/kubelet.service
             3535357 /usr/bin/kubelet --cloud-provider=external --hostname-override=ip-10-0-48-74.ec2.internal --config=/etc/kubernetes/kubelet/config.json --config-dir=/etc/kubernetes/kubelet/config.json.d --kubeconfig=/var/lib/kube>
Jan 10 19:11:21 ip-10-0-48-74.ec2.internal kubelet[3535357]: I0110 19:11:21.512711 3535357 operation_generator.go:664] "MountVolume.MountDevice succeeded for volume \"pvc-0f7e3000-e177-451a-8b37-d30b4ecee2a3\" (UniqueName: \"kubernetes>
Jan 10 19:11:21 ip-10-0-48-74.ec2.internal kubelet[3535357]: I0110 19:11:21.594421 3535357 operation_generator.go:721] "MountVolume.SetUp succeeded for volume \"pvc-0f7e3000-e177-451a-8b37-d30b4ecee2a3\" (UniqueName: \"kubernetes.io/cs>
Jan 10 19:11:21 ip-10-0-48-74.ec2.internal kubelet[3535357]: I0110 19:11:21.804827 3535357 kubelet.go:2545] "SyncLoop (probe)" probe="readiness" status="" pod="prometheus/prometheus-for-amp-prometheus-pushgateway-7697947457-xhk9b"
Jan 10 19:11:21 ip-10-0-48-74.ec2.internal kubelet[3535357]: I0110 19:11:21.805434 3535357 kubelet.go:2545] "SyncLoop (probe)" probe="readiness" status="ready" pod="prometheus/prometheus-for-amp-prometheus-pushgateway-7697947457-xhk9b"
Jan 10 19:11:22 ip-10-0-48-74.ec2.internal kubelet[3535357]: I0110 19:11:22.496383 3535357 kubelet.go:2545] "SyncLoop (probe)" probe="startup" status="unhealthy" pod="prometheus/prometheus-for-amp-server-0"
Jan 10 19:11:22 ip-10-0-48-74.ec2.internal kubelet[3535357]: I0110 19:11:22.629558 3535357 kubelet.go:2545] "SyncLoop (probe)" probe="readiness" status="" pod="longhorn-system/longhorn-manager-s8qfr"
Jan 10 19:11:22 ip-10-0-48-74.ec2.internal kubelet[3535357]: I0110 19:11:22.630592 3535357 kubelet.go:2545] "SyncLoop (probe)" probe="readiness" status="ready" pod="longhorn-system/longhorn-manager-s8qfr"
Jan 10 19:11:22 ip-10-0-48-74.ec2.internal kubelet[3535357]: I0110 19:11:22.954539 3535357 kubelet.go:2545] "SyncLoop (probe)" probe="readiness" status="" pod="prometheus/prometheus-for-amp-server-0"
Jan 10 19:11:22 ip-10-0-48-74.ec2.internal kubelet[3535357]: I0110 19:11:22.954581 3535357 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Jan 10 19:11:22 ip-10-0-48-74.ec2.internal kubelet[3535357]: I0110 19:11:22.955166 3535357 kubelet.go:2545] "SyncLoop (probe)" probe="readiness" status="ready" pod="prometheus/prometheus-for-amp-server-0"
[root@ip-10-0-48-74 ~]#
### VALIDATE WITH KUBECTL
ubuntu@cloud9-sigitp:~$ kubectl describe node ip-10-0-48-74.ec2.internal
Name:               ip-10-0-48-74.ec2.internal
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/instance-type=bmn-sf2.metal-32xl
                    beta.kubernetes.io/os=linux
                    failure-domain.beta.kubernetes.io/region=us-east-1
                    failure-domain.beta.kubernetes.io/zone=us-east-1a
                    feature.node.kubernetes.io/network-sriov.capable=true
                    is_worker=true
                    k8s.io/cloud-provider-aws=99c4e1dcd3f7e7aeb1ccff94f6ff1710
                    kubernetes.io/arch=amd64
                    kubernetes.io/clustername=kinara-dev1
                    kubernetes.io/hostname=ip-10-0-48-74.ec2.internal
                    kubernetes.io/os=linux
                    node.kubernetes.io/instance-type=bmn-sf2.metal-32xl
                    node.longhorn.io/create-default-disk=true
                    storage=longhorn
                    topology.k8s.aws/zone-id=use1-az6
                    topology.kubernetes.io/region=us-east-1
                    topology.kubernetes.io/zone=us-east-1a
Annotations:        alpha.kubernetes.io/provided-node-ip: 10.0.48.74
                    csi.volume.kubernetes.io/nodeid: {"driver.longhorn.io":"ip-10-0-48-74.ec2.internal"}
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 28 Oct 2024 18:37:33 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  ip-10-0-48-74.ec2.internal
  AcquireTime:     <unset>
  RenewTime:       Fri, 10 Jan 2025 19:12:02 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 10 Jan 2025 19:11:20 +0000   Wed, 08 Jan 2025 21:45:35 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 10 Jan 2025 19:11:20 +0000   Wed, 08 Jan 2025 21:45:35 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 10 Jan 2025 19:11:20 +0000   Wed, 08 Jan 2025 21:45:35 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 10 Jan 2025 19:11:20 +0000   Wed, 08 Jan 2025 21:45:35 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:   10.0.48.74
  ExternalIP:   34.207.105.185
  InternalDNS:  ip-10-0-48-74.ec2.internal
  Hostname:     ip-10-0-48-74.ec2.internal
Capacity:
  cpu:                           192
  ephemeral-storage:             3710859656Ki
  hugepages-1Gi:                 32Gi
  hugepages-2Mi:                 0
  kinara.com/bmn-mlx-sriov-pf1:  10
  kinara.com/bmn-mlx-sriov-pf2:  10
  kinara.com/bmn-mlx-sriov-pf3:  10
  kinara.com/bmn-mlx-sriov-pf4:  10
  memory:                        1056600096Ki
  pods:                          737
Allocatable:
  cpu:                           191450m
  ephemeral-storage:             3418854511484
  hugepages-1Gi:                 32Gi
  hugepages-2Mi:                 0
  kinara.com/bmn-mlx-sriov-pf1:  10
  kinara.com/bmn-mlx-sriov-pf2:  10
  kinara.com/bmn-mlx-sriov-pf3:  10
  kinara.com/bmn-mlx-sriov-pf4:  10
  memory:                        1014380576Ki
  pods:                          737
System Info:
  Machine ID:                 ec27f1c4fc6918f4f6a0b746de7a97d8
  System UUID:                ec29c3e5-7265-acf0-9558-ae2470d63d92
  Boot ID:                    3d08fedb-ee44-4a2f-bbd6-8fd5d94a059f
  Kernel Version:             6.1.109-120.189.amzn2023.x86_64
  OS Image:                   Amazon Linux 2023.5.20240819
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  containerd://1.7.20
  Kubelet Version:            v1.30.2-eks-1552ad0
  Kube-Proxy Version:         v1.30.2-eks-1552ad0
ProviderID:                   aws:///us-east-1a/i-0a93eee4e9455f324


7. Root history launch new node
======================================
[root@ip-10-0-62-173 ~]# history
    1  cd
    2  cd /home/ec2-user/
    3  ls
    4  rpm -ivh kernel-6.1.109-120.189.amzn2023.x86_64.rpm
    5  sudo reboot
    6  cd
    7  lsmod | grep mlx
    8  grep MLX /boot/config-6.1.109-120.189.amzn2023.x86_64
    9  sudo modprobe mlx5_core
   10  sudo modprobe mlx5_ib
   11  lsmod | grep mlx
   12  cd
   13  vim /etc/pm/power.d/node-init-script.sh
   14  vi /etc/pm/power.d/node-init-script.sh
   15  vi /etc/pm/power.d/node-init-script.sh
   16  yum update
   17  yum install vim
   18  ping
   19  yum install net-tools -y
   20  yum install -y net-tools tcpdump vim iperf3 iftop ethtool netcat iputils-ping wget curl iproute2 dnsutils telnet git
   21  yum install -y net-tools
   22  yum install -y tcpdump
   23  yum install -y vim
   24  yum install -y iperf
   25  yum install -y iftop
   26  yum install -y ethtool
   27  yum install -y netcat
   28  yum install -y iputils-ping
   29  yum install -y iputils
   30  yum install -y wget
   31  yum install -y curl
   32  yum install -y curl
   33  cur
   34  yum install -y iproute
   35  yum install -y iproute2
   36  yum install -y dnsutils
   37  yum install -y telent
   38  yum install -y telnet
   39  yum install -y git
   40  yum install -y lshw
   41  history
   42  cat /etc/pm/power.d/node-init-script.sh
   43  lsmod | grep mlx
   44  vim /etc/pm/power.d/node-init-script.sh
   45  uname -r
   46  grep MLX /boot/config-6.1.109-120.189.amzn2023.x86_64
   47  uname -r
   48  cd
   49  exit
   50  cd
   51  cd
   52  vim /bin/create-virtual-function.sh
   53  cd
   54  cat /bin/create-virtual-function.sh
   55  vim /etc/systemd/system/createvf.service
   56  cd /etc/systemd/system
   57  ls
   58  cat nodeadm-config.service
   59  cd
   60  cat /bin/create-virtual-function.sh
   61  vim /bin/create-virtual-function.sh
   62  vim /bin/create-virtual-function.sh
   63  cat /bin/create-virtual-function.sh
   64  exit
   65  cd
   66  vim /bin/create-virtual-function.sh
   67  cat /etc/systemd/system/createvf.service
   68  cd
   69  chmod u+x /bin/create-virtual-function.sh
   70  /bin/create-virtual-function.sh
   71  exit
   72  cd /bin/
   73  ls
   74  cat create-virtual-function.sh
   75  vim create-virtual-function.sh
   76  cat create-virtual-function.sh
   77  ./create-virtual-function.sh
   78  ip -d link show
   79  cd
   80  cat /proc/sys/vm/nr_hugepages
   81  echo 32 > /proc/sys/vm/nr_hugepages
   82  cat /etc/sysctl.conf
   83  echo "vm.nr_hugepages=32" >> /etc/sysctl.conf
   84  grep HugePages_Total /proc/meminfo
   85  service kubelet status
   86  service kubelet restart
   87  cd
   88  history

========================================================================================================================
## ENA/ENI testing for EC2 instances in Region
========================================================================================================================
### TX
```
./build/app/dpdk-testpmd -l 32-42 -n 6 -a 0000:00:06.0 --file-prefix sigitp-dpdk-test --socket-mem=4096,4096 --proc-type=auto -- --mbcache=512 --burst=64 --nb-cores=8 --rxq=4 --txq=4 -i --rxd=8192 --txd=256 --forward-mode=txonly --txonly-multi-flow --eth-peer=0,1e:5d:f6:08:f6:b9 --tx-ip=10.0.3.9,10.0.3.166 --max-pkt-len=9000
```
### RX
```
./build/app/dpdk-testpmd -l 32-42 -n 6 -a 0000:00:06.0 --file-prefix sigitp-dpdk-test -- --nb-cores=8 --rxq=4 --txq=4 -i --forward-mode=rxonly --eth-peer=0,1e:5d:f6:08:87:09 --stats-period 5
```
### Set MTU and Packet Size
```
testpmd> show port info 0
testpmd> port config mtu 0 9000
testpmd> set txpkts 750
testpmd> start
testpmd> show port stats 0
```
--
ENA team has some utilities for testing also https://github.com/amzn/amzn-ec2-ena-utilities/tree/main/ena-dts. Though the commits are a bit old.
Also see the sample result https://github.com/amzn/amzn-ec2-ena-utilities/blob/main/ena-dts/RESULTS.md


```
